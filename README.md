# OmniSat: Self-Supervised Modality Fusion for Earth Observation

[![python](https://img.shields.io/badge/-Python_3.8+-blue?logo=python&logoColor=white)](https://github.com/pre-commit/pre-commit)
[![pytorch](https://img.shields.io/badge/PyTorch_2.2+-ee4c2c?logo=pytorch&logoColor=white)](https://pytorch.org/get-started/locally/)
[![lightning](https://img.shields.io/badge/-Lightning_2.2+-792ee5?logo=pytorchlightning&logoColor=white)](https://pytorchlightning.ai/)
[![hydra](https://img.shields.io/badge/Config-Hydra_1.3-89b8cd)](https://hydra.cc/)
[![license](https://img.shields.io/badge/License-MIT-green.svg?labelColor=gray)](https://github.com/ashleve/lightning-hydra-template#license)

[//]: # ([![Paper]&#40;https://img.shields.io/badge/paper-arxiv.1001.2234-B31B1B.svg&#41;]&#40;https://www.nature.com/articles/nature14539&#41;)
[//]: # ([![Conference]&#40;https://img.shields.io/badge/AnyConference-year-4b44ce.svg&#41;]&#40;https://papers.nips.cc/paper/2020&#41;)


Official implementation for
<br>
<br>
[_OmniSat: Self-Supervised Modality Fusion for Earth Observation_](https://arxiv.org)
<br>

## Description

### Abstract

We introduce OmniSat, a novel architecture that exploits the spatial alignment between multiple EO modalities to learn expressive multimodal representations without labels. To demonstrate the advantages of combining modalities of different natures, we augment two existing datasets with new modalities. As demonstrated on three downstream tasks: forestry, land cover classification, and crop mapping. OmniSat can learn rich representations in an unsupervised manner, leading to improved performance in the semi- and fully-supervised settings, even when only one modality is available for inference.

### Datasets

| Dataset name  |             Modalities                   |      Labels         |
| ------------- | ---------------------------------------- | ------------------- |
| PASTIS-HD     | **SPOT 6-7 (1m)** + S1/S2 (30-140 / year)| Crop mapping (0.2m) |
| TreeSatAI-TS  | aerial (0.2m) + **S1/S2 (10-70 / year)** |   forestry (60m)    |
| FLAIR         |   aerial (0.2m) + S2 (20-114 / year)     |  Land cover (0.2m)  |

### Results

F1 Score results on 100% of the training data with all modalities available:

|   F1 Score    | UT&T | Scale-MAE | OmniSat (no pretraining) | OmniSat (with pretraining) |
| ------------- | ---- | --------- | ------------------------ | -------------------------- |
| PASTIS-HD     | 53.5 |   42.2    |           59.1           |          **69.9**          |
| TreeSatAI-TS  | 56.7 |   60.4    |           73.3           |          **74.2**          |
| FLAIR         | 48.8 |   70.0    |           70.0           |          **73.4**          |

F1 Score results on 100% of the training data with only S2 data available:

|   F1 Score    | UT&T | Scale-MAE | OmniSat (no pretraining) | OmniSat (with pretraining) |
| ------------- | ---- | --------- | ------------------------ | -------------------------- |
| PASTIS-HD     | 61.3 |   46.1    |           60.1           |          **70.8**          |
| TreeSatAI-TS  | 57.0 |   31.5    |           49.7           |          **62.9**          |
| FLAIR         | 62.0 |   61.0    |         **65.4**         |          **65.4**          |

OmniSat improves performance even when only one modality is available for inference!

### Efficiency

We report the best performance of different models between TreeSatAI and TreeSatAI-TS, with pre-training and fine-tuning using 100% of labels. The area of the markers is proportional to the training time, broken down in pre-training and fine-tuning when applicable

![](Efficiency.png)

## Project Structure

The directory structure of new project looks like this:

```
â”œâ”€â”€ configs                   <- Hydra configs
â”‚   â”œâ”€â”€ callbacks                <- Callbacks configs
â”‚   â”œâ”€â”€ dataset                  <- Data configs
â”‚   â”œâ”€â”€ debug                    <- Debugging configs
â”‚   â”œâ”€â”€ exp                      <- Experiment configs
â”‚   â”œâ”€â”€ extras                   <- Extra utilities configs
â”‚   â”œâ”€â”€ hparams_search           <- Hyperparameter search configs
â”‚   â”œâ”€â”€ hydra                    <- Hydra configs
â”‚   â”œâ”€â”€ local                    <- Local configs
â”‚   â”œâ”€â”€ logger                   <- Logger configs
â”‚   â”œâ”€â”€ model                    <- Model configs
â”‚   â”œâ”€â”€ paths                    <- Project paths configs
â”‚   â”œâ”€â”€ trainer                  <- Trainer configs
â”‚   â”‚
â”‚   â”œâ”€â”€ config.yaml            <- Main config for training
â”‚   â””â”€â”€ eval.yaml              <- Main config for evaluation
â”‚
â”œâ”€â”€ data                   <- Project data
â”‚
â”œâ”€â”€ logs                   <- Logs generated by hydra and lightning loggers
â”‚
â”œâ”€â”€ src                    <- Source code
â”‚   â”œâ”€â”€ data                     <- Data scripts
â”‚   â”œâ”€â”€ models                   <- Model scripts
â”‚   â”œâ”€â”€ utils                    <- Utility scripts
â”‚   â”‚
â”‚   â”œâ”€â”€ eval.py                  <- Run evaluation
â”‚   â”œâ”€â”€ train_pastis_20.py       <- Run training on 20% pastis dataset
â”‚   â””â”€â”€ train.py                 <- Run training
â”‚
â”œâ”€â”€ .env.example              <- Example of file for storing private environment variables
â”œâ”€â”€ .gitignore                <- List of files ignored by git
â”œâ”€â”€ .project-root             <- File for inferring the position of project root directory
â”œâ”€â”€ environment.yaml          <- File for installing conda environment
â”œâ”€â”€ Makefile                  <- Makefile with commands like `make train` or `make test`
â”œâ”€â”€ pyproject.toml            <- Configuration options for testing and linting
â”œâ”€â”€ requirements.txt          <- File for installing python dependencies
â”œâ”€â”€ setup.py                  <- File for installing project as a package
â””â”€â”€ README.md
```

## Getting the data



## ðŸš€Â Â Quickstart

```bash
# clone project
git clone https://github.com/ashleve/lightning-hydra-template
cd lightning-hydra-template

# [OPTIONAL] create conda environment
conda create -n omni python=3.9
conda activate omni

# install pytorch according to instructions
# https://pytorch.org/get-started/

# install requirements
pip install -r requirements.txt

# Create data folder where you can put your datasets
mkdir data
# Create logs folder
mkdir logs
```

## Usage

Every experience of the paper has its own config. Feel free to explore configs/exp folder

```bash
python src/train.py exp=TSAITS_OmniSAT #to run OmniSAT pretraining on TreeSatAI-TS
#trainer.devices=X to change the number of GPU you want to train on
#trainer.num_workers=16 to change the num_workers available
#dataset.global_batch_size=16 to change global batch size (ie batch size that will be distributed across all GPUS)
#offline=True to run in offline mode from wandb
#max_epochs=1 to change the number maximum of epochs

python src/train.py exp=TSAITS_OmniSAT #to run OmniSAT finetuning on TreeSatAI-TS
#model.name=OmniSAT_MM  to change model name for logging
#partition=1.0 to change the percentage on training data you want to use

# All these parameters and more can be changed from the config file
```

To run 20% experiments on PASTIS-HD, you have to run

```bash
python src/train_pastis_20.py exp=Pastis_ResNet #to run a ResNet on PASTIS-HD
#partition parameter does not change anything on PASTIS-HD
```

## Acknowledgements
- This project was built using [Lightning-Hydra template](https://github.com/ashleve/lightning-hydra-template).
- The original code of [TreeSat](https://git.tu-berlin.de/rsim/treesat_benchmark)
- The transformer structures come from [timm](https://github.com/huggingface/pytorch-image-models)
- The implementation of SatMAE and ScaleMAE come from [USat](https://github.com/stanfordmlgroup/USat)
- The Croma implementation comes from [Croma](https://github.com/antofuller/CROMA)
- The Relative Positional Encoding implementation come from [iRPE](https://github.com/microsoft/Cream/tree/main/iRPE)
- The Pse, ltae, UTAE come from [utae-paps](https://github.com/VSainteuf/utae-paps/tree/main)

<br>